{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  ultralytics import YOLO \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=YOLO(\"best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vivek/work/ML/image_processing/YOLO V8/dataset'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(7990,8000):\n",
    "#     path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "#     result=model.predict(source=path, show=True)\n",
    "#     cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read image and get box vectore\n",
    "path=\"img_0.jpg\"\n",
    "original_image_from_path=cv2.imread(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/vivek/work/ML/image_processing/YOLO V8/dataset/img_0.jpg: 640x448 1 head, 1 body, 69.4ms\n",
      "Speed: 2.4ms preprocess, 69.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    }
   ],
   "source": [
    "result=model.predict(source=path, show=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Results object with attributes:\n",
       "\n",
       "boxes: ultralytics.engine.results.Boxes object\n",
       "keypoints: None\n",
       "masks: None\n",
       "names: {0: 'head', 1: 'body'}\n",
       "obb: None\n",
       "orig_img: array([[[230, 229, 233],\n",
       "        [229, 228, 232],\n",
       "        [227, 226, 230],\n",
       "        ...,\n",
       "        [228, 227, 231],\n",
       "        [224, 223, 227],\n",
       "        [222, 221, 225]],\n",
       "\n",
       "       [[230, 229, 233],\n",
       "        [229, 228, 232],\n",
       "        [227, 226, 230],\n",
       "        ...,\n",
       "        [227, 226, 230],\n",
       "        [224, 223, 227],\n",
       "        [221, 220, 224]],\n",
       "\n",
       "       [[230, 229, 233],\n",
       "        [229, 228, 232],\n",
       "        [227, 226, 230],\n",
       "        ...,\n",
       "        [226, 225, 229],\n",
       "        [223, 222, 226],\n",
       "        [220, 219, 223]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 66,  68,  79],\n",
       "        [ 65,  67,  78],\n",
       "        [ 63,  65,  76],\n",
       "        ...,\n",
       "        [ 87,  88,  98],\n",
       "        [ 91,  92, 102],\n",
       "        [ 97,  98, 108]],\n",
       "\n",
       "       [[ 66,  68,  79],\n",
       "        [ 64,  66,  77],\n",
       "        [ 61,  63,  74],\n",
       "        ...,\n",
       "        [ 92,  93, 103],\n",
       "        [ 93,  94, 104],\n",
       "        [ 94,  95, 105]],\n",
       "\n",
       "       [[ 67,  69,  80],\n",
       "        [ 65,  67,  78],\n",
       "        [ 63,  65,  76],\n",
       "        ...,\n",
       "        [ 96,  97, 107],\n",
       "        [ 92,  93, 103],\n",
       "        [ 88,  89,  99]]], dtype=uint8)\n",
       "orig_shape: (600, 400)\n",
       "path: '/Users/vivek/work/ML/image_processing/YOLO V8/dataset/img_0.jpg'\n",
       "probs: None\n",
       "save_dir: 'runs/detect/predict2'\n",
       "speed: {'preprocess': 2.3970603942871094, 'inference': 69.36502456665039, 'postprocess': 0.30732154846191406}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the boxes value is :tensor([[182.1125, 123.4825, 336.3949, 577.2126],\n",
      "        [232.5658,  43.5120, 308.0630, 121.2043]])\n",
      "{0: 'head', 1: 'body'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"the boxes value is :{result[0].boxes.xyxy}\")\n",
    "print(result[0].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "head_cordinates= result[0].boxes.xyxy[0]\n",
    "body_corrdinates=result[0].boxes.xyxy[1]\n",
    "\n",
    "original_image_from_prediction=result[0].orig_img\n",
    "predicted_frame=cv2.rectangle(original_image_from_path,  (int(body_corrdinates[0]), int(body_corrdinates[1])), (int(body_corrdinates[2]), int(body_corrdinates[3])), (0,255,0), 1)\n",
    "cv2.imshow(\"img_1\",predicted_frame)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_part(img,arr):\n",
    "    \"\"\"\n",
    "    img: original image \n",
    "    arr: cordinates for subdivision\n",
    "    \"\"\"\n",
    "    return img[arr[1]:arr[3],arr[0]:arr[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_0.jpg: 640x448 1 head, 1 body, 163.9ms\n",
      "Speed: 3.0ms preprocess, 163.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     232.57      43.512      308.06       121.2]\n",
      "[     182.11      123.48      336.39      577.21]\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_1.jpg: 640x448 1 head, 1 body, 162.7ms\n",
      "Speed: 3.0ms preprocess, 162.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     102.51      98.725      305.54      593.65]\n",
      "[     149.38      2.1381      244.06       99.27]\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_2.jpg: 640x448 1 head, 1 body, 169.7ms\n",
      "Speed: 3.0ms preprocess, 169.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     80.511           0      187.14      82.681]\n",
      "[     52.123      64.412      221.89      588.18]\n"
     ]
    }
   ],
   "source": [
    "#computing average head and body height\n",
    "head_height=0\n",
    "head_width=0\n",
    "body_height=0\n",
    "body_width=0\n",
    "\n",
    "#fucntion to get the mask of head and body for the corropnding head and body\n",
    "for i in range(0,3):\n",
    "    img_path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    mask_path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\Categories\\img_{i}.png\"\n",
    "    \n",
    "    result=model.predict(source=img_path, show=True)\n",
    "\n",
    "    mask_frame=cv2.imread(mask_path)\n",
    "    img_fram=cv2.imread(img_path)\n",
    "\n",
    "    #extracting corrdinates of head and body from model\n",
    "    headCordinates=np.array(result[0].boxes.cpu().numpy().xyxy[1], dtype=np.uint32)\n",
    "    bodyCosrdinates=np.array(result[0].boxes.cpu().numpy().xyxy[0], dtype=np.uint32)\n",
    "\n",
    "    print(result[0].boxes.cpu().numpy().xyxy[1])\n",
    "    print(result[0].boxes.cpu().numpy().xyxy[0])\n",
    "    \n",
    "    head_height+=headCordinates[3]-headCordinates[1]\n",
    "    head_width+=headCordinates[2]-headCordinates[0]\n",
    "\n",
    "    body_height+=bodyCosrdinates[3]-bodyCosrdinates[1]\n",
    "    body_width+=bodyCosrdinates[2]-bodyCosrdinates[0]    \n",
    "\n",
    "    #showing the head and body original image\n",
    "    cv2.imshow(\"head img\",get_img_part(img_fram,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(img_fram,bodyCosrdinates))\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "    # #showing the head and body mask image\n",
    "    cv2.imshow(\"head img\",get_img_part(mask_frame,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(mask_frame,bodyCosrdinates))\n",
    "\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218.33333333333334\n",
      "128.66666666666666\n",
      "358.3333333333333\n",
      "139.33333333333334\n"
     ]
    }
   ],
   "source": [
    "print(head_height/3)\n",
    "print(head_width/3)\n",
    "print(body_height/3)\n",
    "print(body_width/3)\n",
    "\n",
    "# head->240 by 140\n",
    "# body->440 by 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.2185) tensor(-47.2631)\n"
     ]
    }
   ],
   "source": [
    "print(x1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"img\",result[0].orig_img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     109.76      153.17      266.85      554.03]\n",
      "[     164.52      90.647       227.3      169.16]\n"
     ]
    }
   ],
   "source": [
    "for res in result:\n",
    "    a=res.boxes.cpu().numpy()\n",
    "    #getting the corrdinates of bounding box for classes\n",
    "    xyxys=a.xyxy\n",
    "    c=False\n",
    "    for xyxy in xyxys:\n",
    "        if c:\n",
    "            True\n",
    "            continue\n",
    "        print(xyxy)\n",
    "        img1=cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "    cv2.imshow(\"img_\", img1)\n",
    "    cv2.waitKey(0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109 153 266 554]\n",
      "[164  90 227 169]\n"
     ]
    }
   ],
   "source": [
    "arr=np.array(xyxys[0],dtype=np.uint16)\n",
    "arr1=np.array(xyxys[1],dtype=np.uint16)\n",
    "\n",
    "print(arr)\n",
    "print(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_part(img,arr):\n",
    "    \"\"\"\n",
    "    img: original image \n",
    "    arr: cordinates for subdivision\n",
    "    \"\"\"\n",
    "    return img[arr[1]:arr[3],arr[0]:arr[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 63, 3)\n"
     ]
    }
   ],
   "source": [
    "body=frame[arr1[1]:arr1[3],arr1[0]:arr1[2]]\n",
    "head=frame[arr1[1]:arr1[3],arr1[0]:arr1[2]]\n",
    "print(head.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"temp img\",head)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"temp img\",get_img_part(frame,arr1))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temparr=np.array([\n",
    "    [1,2,3,4,5,6],\n",
    "    [7,8,9,10,11,12],\n",
    "    [13,14,15,16,17,18]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  8],\n",
       "       [13, 14]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temparr[1:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7998.jpg: 640x448 1 head, 1 body, 161.1ms\n",
      "Speed: 4.0ms preprocess, 161.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     176.63      85.494      233.31      163.06]\n",
      "[     134.03      161.07      276.26      595.02]\n",
      "(600, 400, 3)\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7999.jpg: 640x448 1 head, 1 body, 155.6ms\n",
      "Speed: 4.5ms preprocess, 155.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[      133.2      159.75      270.86      557.78]\n",
      "[     197.77      100.55      260.26       167.7]\n",
      "(600, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(7998,8000):\n",
    "    path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    original_image=cv2.imread(path)\n",
    "    result=model.predict(source=path, show=True)\n",
    "    frame=result[0].orig_img\n",
    "    a=result[0].boxes.cpu().numpy()\n",
    "    #getting the corrdinates of bounding box for classes\n",
    "    xyxys=a.xyxy\n",
    "    c=False\n",
    "    for xyxy in xyxys:\n",
    "        if c:\n",
    "            True\n",
    "            continue\n",
    "        print(xyxy)\n",
    "        img1=cv2.rectangle(frame, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "        framed_orginal_image=cv2.rectangle(original_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "    cv2.imshow(\"img_outfile_from_NET\", img1)\n",
    "    cv2.imshow(\"img_outfile_from_original\", framed_orginal_image)\n",
    "    print(img1.shape)\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7998.jpg: 640x448 1 head, 1 body, 162.5ms\n",
      "Speed: 3.0ms preprocess, 162.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "(600, 400, 3)\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_7999.jpg: 640x448 1 head, 1 body, 163.6ms\n",
      "Speed: 5.0ms preprocess, 163.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "(600, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(7998,8000):\n",
    "    #reading the original image from path\n",
    "    path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    original_image=cv2.imread(path)\n",
    "\n",
    "    #passing the image to the model\n",
    "    result=model.predict(source=path, show=True)\n",
    "    a=result[0].boxes.cpu().numpy()\n",
    "\n",
    "    #getting the corrdinates of bounding box for classes\n",
    "    xyxys=a.xyxy\n",
    "\n",
    "    #building the bounding box\n",
    "    for xyxy in xyxys:\n",
    "        framed_orginal_image=cv2.rectangle(original_image, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0,255,0), 1)\n",
    "\n",
    "    cv2.imshow(\"img_outfile_from_original\", framed_orginal_image)\n",
    "    cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_0.jpg: 640x448 1 head, 1 body, 155.6ms\n",
      "Speed: 3.0ms preprocess, 155.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_1.jpg: 640x448 1 head, 1 body, 159.9ms\n",
      "Speed: 2.0ms preprocess, 159.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    }
   ],
   "source": [
    "# fuction to get original image preditciton and its corrosponding head and body img\n",
    "for i in range(0,2):\n",
    "    path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    result=model.predict(source=path, show=True)\n",
    "    frame=cv2.imread(path)\n",
    "    headCordinates=np.array(result[0].boxes.cpu().numpy().xyxy[1], dtype=np.uint32)\n",
    "    bodyCosrdinates=np.array(result[0].boxes.cpu().numpy().xyxy[0], dtype=np.uint32)\n",
    "    cv2.imshow(\"head img\",get_img_part(frame,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(frame,bodyCosrdinates))\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_0.jpg: 640x448 1 head, 1 body, 178.0ms\n",
      "Speed: 4.0ms preprocess, 178.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     232.57      43.512      308.06       121.2]\n",
      "[     182.11      123.48      336.39      577.21]\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_1.jpg: 640x448 1 head, 1 body, 169.1ms\n",
      "Speed: 3.0ms preprocess, 169.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     102.51      98.725      305.54      593.65]\n",
      "[     149.38      2.1381      244.06       99.27]\n",
      "\n",
      "image 1/1 D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_2.jpg: 640x448 1 head, 1 body, 164.6ms\n",
      "Speed: 3.0ms preprocess, 164.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "[     80.511           0      187.14      82.681]\n",
      "[     52.123      64.412      221.89      588.18]\n"
     ]
    }
   ],
   "source": [
    "#fucntion to get the mask of head and body for the corropnding head and body\n",
    "for i in range(0,3):\n",
    "    img_path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\input\\img_{i}.jpg\"\n",
    "    mask_path=f\"D:\\work\\ML\\image_processing\\YOLO V8\\dataset\\Categories\\img_{i}.png\"\n",
    "    \n",
    "    result=model.predict(source=img_path, show=True)\n",
    "\n",
    "    mask_frame=cv2.imread(mask_path)\n",
    "    img_fram=cv2.imread(img_path)\n",
    "\n",
    "    #extracting corrdinates of head and body from model\n",
    "    headCordinates=np.array(result[0].boxes.cpu().numpy().xyxy[1], dtype=np.uint32)\n",
    "    bodyCosrdinates=np.array(result[0].boxes.cpu().numpy().xyxy[0], dtype=np.uint32)\n",
    "\n",
    "\n",
    "    #showing the head and body original image\n",
    "    cv2.imshow(\"head img\",get_img_part(img_fram,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(img_fram,bodyCosrdinates))\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "    # #showing the head and body mask image\n",
    "    cv2.imshow(\"head img\",get_img_part(mask_frame,headCordinates))\n",
    "    cv2.imshow(\"body img\",get_img_part(mask_frame,bodyCosrdinates))\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# head parsing network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image form above model will be resized to (480, ,280 , 3)\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def fcn8(input_shape=(320, 192, 3), num_classes=21):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Fully Convolutional Layers (FC Layers)\n",
    "    x = tf.keras.layers.Conv2D(4096, (7, 7), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Conv2D(4096, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Classification layer (output layer)\n",
    "    x = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax', padding='same')(x)\n",
    "\n",
    "    # Upsampling for FCN-8\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(16, 16), strides=(8, 8), padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "model = fcn8(input_shape=(320, 192, 3), num_classes=21)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# body parsing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fcn8(input_shape=(640, 384, 3), num_classes=21):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Fully Convolutional Layers (FC Layers)\n",
    "    x = tf.keras.layers.Conv2D(4096, (7, 7), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.Conv2D(4096, (1, 1), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Classification layer (output layer)\n",
    "    x = tf.keras.layers.Conv2D(num_classes, (1, 1), activation='softmax', padding='same')(x)\n",
    "\n",
    "    # Upsampling for FCN-8\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(4, 4), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_classes, kernel_size=(16, 16), strides=(8, 8), padding='same')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "model = fcn8(input_shape=(640, 384, 3), num_classes=21)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your FCN-8 model (assuming you've already defined `fcn8` function as provided earlier)\n",
    "model = fcn8(input_shape=(640, 384, 3), num_classes=21)\n",
    "\n",
    "# Define your loss function (e.g., categorical crossentropy for multi-class segmentation)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Define optimizer with momentum and weight decay\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.9, decay=0.0005)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Prepare dummy data (replace with your actual data loading and preprocessing)\n",
    "import numpy as np\n",
    "x_train = np.random.randn(100, 640, 384, 3)  # example random input data\n",
    "y_train = np.random.randint(0, 21, size=(100, 640, 384))  # example random labels\n",
    "\n",
    "# Dummy training loop (replace with your actual training loop)\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch, training=True)\n",
    "            loss_value = loss_fn(y_batch, logits)\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # Print batch loss (optional)\n",
    "        print(f\"  Batch {i//batch_size+1}/{len(x_train)//batch_size}, Loss: {loss_value.numpy():.4f}\")\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "# Replace x_test and y_test with your test data\n",
    "# loss, accuracy = model.evaluate(x_test, y_test)\n",
    "# print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
